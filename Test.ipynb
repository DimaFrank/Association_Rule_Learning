{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+exaWIYETtOP3jHXs6oUa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DimaFrank/Association_Rule_Learning/blob/test/Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZpkMk--GvcO",
        "outputId": "66149a9e-65ee-4ac9-f967-282820d89198"
      },
      "execution_count": 348,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.3.1)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 349,
      "metadata": {
        "id": "_l8HsKQeGmCX"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import array, col, concat_ws, udf, array_remove, size, window, to_timestamp, date_format, concat, lit, collect_list, desc, sort_array, array_contains, array_intersect\n",
        "from pyspark.sql import SparkSession \n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resource:\n",
        "\n",
        "https://www.softwaretestinghelp.com/apriori-algorithm/"
      ],
      "metadata": {
        "id": "tAmi45tkbF7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"user_ct_test\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "jwy-ox42HN6u"
      },
      "execution_count": 350,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TABLE-1\n",
        "rdd = spark.sparkContext.parallelize(\n",
        "    [Row(\"T1\", ['I1', 'I2', 'I3']),\n",
        "     Row(\"T2\", ['I4', 'I3', 'I2']),\n",
        "     Row(\"T3\", ['I4', 'I5']), \n",
        "     Row(\"T4\", ['I1', 'I2', 'I4']),\n",
        "     Row(\"T5\", ['I1', 'I2', 'I3', 'I5']),\n",
        "     Row(\"T6\", ['I1', 'I2', 'I3', 'I4']),   \n",
        "     ]\n",
        ")\n",
        "schema = StructType([\n",
        "    StructField(\"Tid\", StringType(), True),\n",
        "    StructField(\"Basket\", ArrayType(StringType(), True), True)\n",
        "])\n",
        "df = spark.createDataFrame(rdd, schema)\n",
        "df = df.withColumn('size',size(col('Basket')))\n",
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_H7V1BuGuly",
        "outputId": "9722caf6-9dc0-497e-9d41-e836253a7729"
      },
      "execution_count": 351,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------+----+\n",
            "|Tid|          Basket|size|\n",
            "+---+----------------+----+\n",
            "| T1|    [I1, I2, I3]|   3|\n",
            "| T2|    [I4, I3, I2]|   3|\n",
            "| T3|        [I4, I5]|   2|\n",
            "| T4|    [I1, I2, I4]|   3|\n",
            "| T5|[I1, I2, I3, I5]|   4|\n",
            "| T6|[I1, I2, I3, I4]|   4|\n",
            "+---+----------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = df.select(\"Basket\",sort_array(\"Basket\",asc=True).alias('array_sorted'))\n",
        "df_test.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuNQ8UFwINDC",
        "outputId": "d6b60aac-56d5-4e91-b423-fe8dc1f2ad45"
      },
      "execution_count": 352,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----------------+\n",
            "|          Basket|    array_sorted|\n",
            "+----------------+----------------+\n",
            "|    [I1, I2, I3]|    [I1, I2, I3]|\n",
            "|    [I4, I3, I2]|    [I2, I3, I4]|\n",
            "|        [I4, I5]|        [I4, I5]|\n",
            "|    [I1, I2, I4]|    [I1, I2, I4]|\n",
            "|[I1, I2, I3, I5]|[I1, I2, I3, I5]|\n",
            "|[I1, I2, I3, I4]|[I1, I2, I3, I4]|\n",
            "+----------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_sup=3"
      ],
      "metadata": {
        "id": "JYvntACd0YbF"
      },
      "execution_count": 353,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TABLE-2\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "F1 = (df_test\n",
        "           .withColumn(\"explode\", F.explode(\"array_sorted\"))\n",
        "           .groupBy(\"explode\")\n",
        "           .count()\n",
        "           .orderBy(F.desc(\"count\")))\n",
        "F1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdNmAafB0o5_",
        "outputId": "126cbdef-9c62-4b42-d3e1-52455974c2b7"
      },
      "execution_count": 354,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|explode|count|\n",
            "+-------+-----+\n",
            "|     I2|    5|\n",
            "|     I4|    4|\n",
            "|     I3|    4|\n",
            "|     I1|    4|\n",
            "|     I5|    2|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TABLE-3\n",
        "F2 = F1.filter(col('count')>=min_sup)\n",
        "F2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5lvSZap0a1R",
        "outputId": "696d586a-6a9c-4fbe-b204-cf77df9ef141"
      },
      "execution_count": 355,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|explode|count|\n",
            "+-------+-----+\n",
            "|     I2|    5|\n",
            "|     I4|    4|\n",
            "|     I1|    4|\n",
            "|     I3|    4|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# All possible combinations\n",
        "\n",
        "lst = [str(i.explode) for i in F2.select(\"explode\").collect()]\n",
        "print(lst)\n",
        "print('All Combinations: \\n')\n",
        "\n",
        "def create_possible_combinations(all_items, k):\n",
        "    # This functions gets a list of items and k, and returns all possible k-item combinations.\n",
        "    # all_items --> <list>\n",
        "    # k         --> <int>\n",
        "    res_tmp = set([])\n",
        "    for subset in itertools.combinations(all_items, k):       \n",
        "       res_tmp.add((subset))\n",
        "\n",
        "    res_lst = list(res_tmp)\n",
        "    result = [list(res_lst[i]) for i in range(len(res_lst))]\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "create_possible_combinations(lst,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_7IP3gf1HGE",
        "outputId": "f2bf18d5-ca27-4427-ee69-012f9738202d"
      },
      "execution_count": 356,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I2', 'I4', 'I3', 'I1']\n",
            "All Combinations: \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['I2', 'I1'],\n",
              " ['I2', 'I4'],\n",
              " ['I3', 'I1'],\n",
              " ['I4', 'I3'],\n",
              " ['I2', 'I3'],\n",
              " ['I4', 'I1']]"
            ]
          },
          "metadata": {},
          "execution_count": 356
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an empty DataFrame\n",
        "emp_RDD = spark.sparkContext.emptyRDD()\n",
        "columns1 = StructType([StructField('Item', ArrayType(StringType()), False),\n",
        "                       StructField('count', LongType(), False)])\n",
        "first_df = spark.createDataFrame(data=emp_RDD,\n",
        "                                         schema=columns1)\n",
        "\n",
        "\n",
        "for row in create_possible_combinations(lst,2):\n",
        "    # print(row)\n",
        "    res=df_test.withColumn(\"NewColumn\", F.array([F.lit(x) for x in row]))\n",
        "    res= res.select('array_sorted', size(array_intersect(res.array_sorted, res.NewColumn)).alias('Intersect'))\n",
        "    res = res.filter(col('Intersect')>=2).count()\n",
        "    # print(row, 'res=', res)\n",
        "    columns=['Item','count']\n",
        "    newRow = spark.createDataFrame([(row, res)], columns)\n",
        "    first_df = first_df.union(newRow).filter(col('count')>=min_sup)\n",
        "    \n",
        "\n",
        "first_df.show()  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "magqwgmw1ZGG",
        "outputId": "02d53ca6-424e-4bb4-9cc7-98206b89a940"
      },
      "execution_count": 357,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|    Item|count|\n",
            "+--------+-----+\n",
            "|[I2, I1]|    4|\n",
            "|[I2, I4]|    3|\n",
            "|[I3, I1]|    3|\n",
            "|[I2, I3]|    4|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for row in create_possible_combinations(lst,3):\n",
        "    # print(row)\n",
        "    res=df_test.withColumn(\"NewColumn\", F.array([F.lit(x) for x in row]))\n",
        "    res= res.select('array_sorted', size(array_intersect(res.array_sorted, res.NewColumn)).alias('Intersect'))\n",
        "    res = res.filter(col('Intersect')>=3).count()\n",
        "    # print(row, 'res=', res)\n",
        "    columns=['Item','count']\n",
        "    newRow = spark.createDataFrame([(row, res)], columns)\n",
        "    first_df = first_df.union(newRow).filter(col('count')>=min_sup)\n",
        "\n",
        "\n",
        "first_df.show()  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is4DWHZk1ZN1",
        "outputId": "a87bd4ae-6218-4241-cf9e-b4539ad948f7"
      },
      "execution_count": 358,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----+\n",
            "|        Item|count|\n",
            "+------------+-----+\n",
            "|    [I2, I1]|    4|\n",
            "|    [I2, I4]|    3|\n",
            "|    [I3, I1]|    3|\n",
            "|    [I2, I3]|    4|\n",
            "|[I2, I3, I1]|    3|\n",
            "+------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Algorithm"
      ],
      "metadata": {
        "id": "XKJ-rz6l6S32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "\n",
        "def build_support(sessions_data, items, min_support):\n",
        "\n",
        "    # sessions_data --> <DataFrame> Name of dataset contain sessions\n",
        "    # items         --> <ArrayType(StringType())> Column name that contain items\n",
        "    # min_support   --> <float> between 0 and 1\n",
        "\n",
        "\n",
        "    # Sort item set\n",
        "    data = sessions_data.select(items,sort_array(items,asc=True).alias('array'))\n",
        "\n",
        "    # Create F1 + Filter by min support:\n",
        "    F1 = (data.withColumn(\"explode\", F.explode(\"array\"))\n",
        "          .groupBy(\"explode\")\n",
        "          .count()\n",
        "          .orderBy(F.desc(\"count\"))).filter(col('count')>min_support)\n",
        "\n",
        "    #Create all possible item combinations\n",
        "    lst = [str(i.explode) for i in F1.select(\"explode\").collect()]\n",
        "\n",
        "\n",
        "    # Create an empty DataFrame\n",
        "    emp_RDD = spark.sparkContext.emptyRDD()\n",
        "    columns1 = StructType([StructField('Item', ArrayType(StringType()), False),\n",
        "                          StructField('Support', LongType(), False)])\n",
        "    Support = spark.createDataFrame(data=emp_RDD,\n",
        "                                            schema=columns1)\n",
        "    \n",
        "    # Create Support\n",
        "    for k in range(1,4):\n",
        "        combinations = create_possible_combinations(lst,k)\n",
        "        for j in range(len(combinations)):\n",
        "            # print(combinations[j])\n",
        "            row = sorted(combinations[j])                   \n",
        "            res = sessions_data.withColumn(\"NewColumn\", F.array([F.lit(x) for x in row]))\n",
        "            res = res.select(items,'NewColumn', size(array_intersect(res.Basket, res.NewColumn)).alias('Intersect'))\n",
        "            res = res.filter(col('Intersect')>=k).count()\n",
        "            # res.show()\n",
        "            columns = ['Item','Support']\n",
        "            newRow = spark.createDataFrame([(row, res)], columns)\n",
        "            Support = Support.union(newRow).filter(col('Support')>=min_sup)\n",
        "\n",
        "\n",
        "\n",
        "    return Support"
      ],
      "metadata": {
        "id": "-VTHhlzZf8DO"
      },
      "execution_count": 451,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "support = build_support(df, 'Basket', 3)\n",
        "support.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSvrFHtA8kEq",
        "outputId": "cc5f5bd9-726f-477e-db09-5119cb6c84bc"
      },
      "execution_count": 452,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------+\n",
            "|        Item|Support|\n",
            "+------------+-------+\n",
            "|        [I2]|      5|\n",
            "|        [I3]|      4|\n",
            "|        [I1]|      4|\n",
            "|        [I4]|      4|\n",
            "|    [I1, I2]|      4|\n",
            "|    [I2, I4]|      3|\n",
            "|    [I1, I3]|      3|\n",
            "|    [I2, I3]|      4|\n",
            "|[I1, I2, I3]|      3|\n",
            "+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxoZh7ND8rOq",
        "outputId": "3be21384-12b3-4d08-a04a-a20237304074"
      },
      "execution_count": 450,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------+----+\n",
            "|Tid|          Basket|size|\n",
            "+---+----------------+----+\n",
            "| T1|    [I1, I2, I3]|   3|\n",
            "| T2|    [I4, I3, I2]|   3|\n",
            "| T3|        [I4, I5]|   2|\n",
            "| T4|    [I1, I2, I4]|   3|\n",
            "| T5|[I1, I2, I3, I5]|   4|\n",
            "| T6|[I1, I2, I3, I4]|   4|\n",
            "+---+----------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def association_rule_mining():\n",
        "  return "
      ],
      "metadata": {
        "id": "mjWOCEm6qw2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qv-YwVbGqw5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gpW4qvLVqw7a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}